{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Manvi\n",
      "[nltk_data]     Mehar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Manvi Mehar\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import tensorflow as tf   \n",
    "import numpy as np\n",
    "import tflearn            \n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intents.json\") as json_data: \n",
    "    intents = json.load(json_data)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "documents = []\n",
    "classes = []\n",
    "\n",
    "\n",
    "ignore = [\"?\"]\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        \n",
    "        w = nltk.word_tokenize(pattern) \n",
    "        \n",
    "        words.extend(w) \n",
    "        \n",
    "        documents.append((w, intent[\"tag\"]))\n",
    "        \n",
    "        if intent[\"tag\"] not in classes:      \n",
    "            classes.append(intent[\"tag\"])  \n",
    "            #print(classes)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 Documents \n",
      "\n",
      "37 Classes \n",
      "\n",
      "118 Stemmed Words \n"
     ]
    }
   ],
   "source": [
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]  \n",
    "words = sorted(list(set(words)))  \n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents),\"Documents \\n\")\n",
    "print(len(classes),\"Classes \\n\")\n",
    "print(len(words), \"Stemmed Words \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = [] \n",
    "\n",
    "    pattern_words = doc[0] \n",
    "    \n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]  \n",
    "    \n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] =1 \n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manvi Mehar\\AppData\\Local\\Temp\\ipykernel_24884\\451365208.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training) #Converting training data into numpy array\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training) \n",
    "training = np.array(training) \n",
    "\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 13999  | total loss: \u001b[1m\u001b[32m0.18566\u001b[0m\u001b[0m | time: 0.031s\n",
      "| Adam | epoch: 1000 | loss: 0.18566 - acc: 0.9904 -- iter: 104/106\n",
      "Training Step: 14000  | total loss: \u001b[1m\u001b[32m0.16901\u001b[0m\u001b[0m | time: 0.032s\n",
      "| Adam | epoch: 1000 | loss: 0.16901 - acc: 0.9914 -- iter: 106/106\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Manvi Mehar\\OneDrive - The University of Colorado Denver\\AI-Chatbot-master\\AI-Chatbot-master\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "#Building our own Neural Network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_dir=\"tflearn_logs\") \n",
    "\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True) \n",
    "model.save(\"model.tflearn\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump({\"words\":words, \"classes\":classes, \"train_x\":train_x, \"train_y\":train_y}, open(\"training_data\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"training_data\",\"rb\"))\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"intents.json\") as json_data:\n",
    "    intents = json.load(json_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Manvi Mehar\\OneDrive - The University of Colorado Denver\\AI-Chatbot-master\\AI-Chatbot-master\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "model.load(\"./model.tflearn\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    \n",
    "    \n",
    "    sentence_words = nltk.word_tokenize(sentence) \n",
    "    \n",
    "    sentence_words= [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words, show_details=False):\n",
    "    \n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    \n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(\"Found in bag: %s\"% w)\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {} \n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "def classify(sentence):\n",
    "    \n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    \n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    \n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    \n",
    "    return return_list\n",
    "\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    \n",
    "    if results:\n",
    "        \n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                \n",
    "                if i['tag'] == results[0][0]:\n",
    "                    \n",
    "                    if 'context_set' in i:\n",
    "                        if show_details: print ('context:', i['context_set'])\n",
    "                        context[userID] = i['context_set']\n",
    "\n",
    "                    if not 'context_filter' in i or \\\n",
    "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('tag:', i['tag'])\n",
    "                        \n",
    "                        return print(random.choice(i['responses']))\n",
    "\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our working hours are 9AM to 9PM every day\n"
     ]
    }
   ],
   "source": [
    "response(\"What are your working hours?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We provide Web Penetration Testing,Android Penetration Testing,Docker Penetration Testing,Vulnerability Assessment,Cyber Crime investigation and many more services.\n"
     ]
    }
   ],
   "source": [
    "response(\"What services do you provide?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penetration Testing is the process of finding vulnerabilities on the target. In this case, the organization would have set up all the security measures they could think of and would want to test if there is any other way that their system/network can be hacked.\n"
     ]
    }
   ],
   "source": [
    "response(\"What is Penetration?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vulnerability Assessment is the process of finding flaws on the target. Here, the organization knows that their system/network has flaws or weaknesses and want to find these flaws and prioritize the flaws for fixing.\n"
     ]
    }
   ],
   "source": [
    "response(\"What is vulnerability assessment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye! Come back again soon.\n"
     ]
    }
   ],
   "source": [
    "response(\"bye\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
